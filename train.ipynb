{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T04:05:04.755789Z","iopub.status.busy":"2023-10-30T04:05:04.755414Z","iopub.status.idle":"2023-10-30T04:05:04.760878Z","shell.execute_reply":"2023-10-30T04:05:04.759760Z","shell.execute_reply.started":"2023-10-30T04:05:04.755759Z"},"trusted":true},"outputs":[],"source":["dataset_id = \"dat_pred\"\n","from huggingface_hub import notebook_login\n","# notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install evaluate\n","!pip install rouge_score"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T04:05:04.765468Z","iopub.status.busy":"2023-10-30T04:05:04.765214Z","iopub.status.idle":"2023-10-30T04:05:06.185120Z","shell.execute_reply":"2023-10-30T04:05:06.184047Z","shell.execute_reply.started":"2023-10-30T04:05:04.765445Z"},"trusted":true},"outputs":[],"source":["## Need to put in <hugging_face_key>\n","!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('<hugging_face_key>')\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T04:34:10.468704Z","iopub.status.busy":"2023-10-30T04:34:10.468308Z","iopub.status.idle":"2023-10-30T04:34:11.918931Z","shell.execute_reply":"2023-10-30T04:34:11.918014Z","shell.execute_reply.started":"2023-10-30T04:34:10.468671Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","model_id=\"google/flan-t5-large\"\n","\n","# Load tokenizer of FLAN-t5\n","tokenizer = AutoTokenizer.from_pretrained(model_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:37:48.444931Z","iopub.status.busy":"2023-10-30T05:37:48.444015Z","iopub.status.idle":"2023-10-30T05:37:48.465194Z","shell.execute_reply":"2023-10-30T05:37:48.464160Z","shell.execute_reply.started":"2023-10-30T05:37:48.444897Z"},"trusted":true},"outputs":[],"source":["#read json\n","import json\n","import pandas as pd\n","from datasets import Dataset\n","\n","with open('/kaggle/input/salient/test_data.json') as f:\n","    test = json.load(f)\n","with open('/kaggle/input/salient-train/chatgpt_gen_date.json') as f:\n","    train = json.load(f)\n","    \n","import random\n","random.shuffle(train)\n","random.shuffle(test)\n","\n","print(\"original train size = \", len(train), \":: original test size = \", len(test))\n","train.extend(test[:50])\n","data_train = train\n","data_test = test[50:]\n","print(\"train size = \", len(data_train), \":: test size = \", len(data_test))\n","\n","data_train_processed = []\n","data_test_processed = []\n","# base_prompt = \"\"\"Given the above transcript and today's day and date, tell me after how many days will customer be able to pay?\n","# Return 'NA' if its not possible to infer that. Just output the number of days or 'NA' in your response and nothing else.\"\"\"\n","base_prompts_list = {\n","    \"label\": \"Given the above transcript and today's day and date, give me the date when the customer is expected to make their payment in the format 'dd/mm/yyyy'. Return 'NA' if its not possible to infer this information from the conversation. just return the date or NA and nothing else.\", \n","    \"days_diff\": \"Given the above transcript and today's day and date, give me the number of days after which the customer will be able to pay. Return 0 if its not possible to infer this information from the conversation.Just return the number of days or 0 if not inferrable and nothing else.\"\n","}\n","label_type_list = ['label', 'days_diff']\n","\n","label_type = label_type_list[1] \n","base_prompt = base_prompts_list[label_type]\n","\n","for d in data_train:\n","    data_train_processed.append(\n","        {\n","            'prompt': d['conversation'] + \"\\n todays date (dd/mm/yyyy) = \" + d['conversation_date'] + \"\\n\" + base_prompt,\n","            'label': str(d[label_type])\n","        }\n","    )\n","for d in data_test:\n","    data_test_processed.append(\n","        {\n","            'prompt': d['conversation'] + \"\\n todays date (dd/mm/yyyy) = \" + d['conversation_date'] + \"\\n\" + base_prompt,\n","            'label': str(d[label_type])\n","        }\n","    )\n","\n","data_train_processed = Dataset.from_pandas(pd.DataFrame(data=data_train_processed))\n","data_test_processed = Dataset.from_pandas(pd.DataFrame(data=data_test_processed))\n","\n","# dataset = {\"train\": data_train_processed, \"test\":data_test_processed}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:37:52.111754Z","iopub.status.busy":"2023-10-30T05:37:52.110935Z","iopub.status.idle":"2023-10-30T05:37:52.239643Z","shell.execute_reply":"2023-10-30T05:37:52.238513Z","shell.execute_reply.started":"2023-10-30T05:37:52.111718Z"},"trusted":true},"outputs":[],"source":["from datasets import concatenate_datasets\n","\n","# The maximum total input sequence length after tokenization. \n","# Sequences longer than this will be truncated, sequences shorter will be padded.\n","tokenized_inputs = concatenate_datasets([data_train_processed, data_train_processed]).map(lambda x: tokenizer(x[\"prompt\"], truncation=True), batched=True, remove_columns=[\"prompt\", \"label\"])\n","max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n","print(f\"Max source length: {max_source_length}\")\n","\n","# The maximum total sequence length for target text after tokenization. \n","# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n","tokenized_targets = concatenate_datasets([data_train_processed, data_train_processed]).map(lambda x: tokenizer(x[\"label\"], truncation=True), batched=True, remove_columns=[\"prompt\", \"label\"])\n","max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n","print(f\"Max target length: {max_target_length}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:37:59.214069Z","iopub.status.busy":"2023-10-30T05:37:59.213647Z","iopub.status.idle":"2023-10-30T05:37:59.278155Z","shell.execute_reply":"2023-10-30T05:37:59.277256Z","shell.execute_reply.started":"2023-10-30T05:37:59.214036Z"},"trusted":true},"outputs":[],"source":["def preprocess_function(sample,padding=\"max_length\"):\n","    # add prefix to the input for t5\n","    inputs = [item for item in sample[\"prompt\"]]\n","\n","    # tokenize inputs\n","    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n","\n","    # Tokenize targets with the `text_target` keyword argument\n","    labels = tokenizer(text_target=sample[\"label\"], max_length=max_target_length, padding=padding, truncation=True)\n","\n","    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n","    # padding in the loss.\n","    if padding == \"max_length\":\n","        labels[\"input_ids\"] = [\n","            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n","        ]\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","tokenized_dataset = data_train_processed.map(preprocess_function, batched=True, remove_columns=[\"prompt\", \"label\"])\n","print(f\"Keys of tokenized dataset: {list(tokenized_dataset.features)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:38:00.505069Z","iopub.status.busy":"2023-10-30T05:38:00.504225Z","iopub.status.idle":"2023-10-30T05:38:26.495469Z","shell.execute_reply":"2023-10-30T05:38:26.494549Z","shell.execute_reply.started":"2023-10-30T05:38:00.505031Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForSeq2SeqLM\n","\n","# load model from the hub\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:38:26.498199Z","iopub.status.busy":"2023-10-30T05:38:26.497816Z","iopub.status.idle":"2023-10-30T05:38:27.205508Z","shell.execute_reply":"2023-10-30T05:38:27.204618Z","shell.execute_reply.started":"2023-10-30T05:38:26.498164Z"},"trusted":true},"outputs":[],"source":["import evaluate\n","import nltk\n","import numpy as np\n","from nltk.tokenize import sent_tokenize\n","nltk.download(\"punkt\")\n","\n","# Metric\n","metric = evaluate.load(\"rouge\")\n","\n","# helper function to postprocess text\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [label.strip() for label in labels]\n","\n","    # rougeLSum expects newline after each sentence\n","    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n","    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n","\n","    return preds, labels\n","\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Some simple post-processing\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    result = {k: round(v * 100, 4) for k, v in result.items()}\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:38:27.206879Z","iopub.status.busy":"2023-10-30T05:38:27.206609Z","iopub.status.idle":"2023-10-30T05:38:27.211746Z","shell.execute_reply":"2023-10-30T05:38:27.210832Z","shell.execute_reply.started":"2023-10-30T05:38:27.206855Z"},"trusted":true},"outputs":[],"source":["from transformers import DataCollatorForSeq2Seq\n","\n","# we want to ignore tokenizer pad token in the loss\n","label_pad_token_id = -100\n","# Data collator\n","data_collator = DataCollatorForSeq2Seq(\n","    tokenizer,\n","    model=model,\n","    label_pad_token_id=label_pad_token_id,\n","    pad_to_multiple_of=8\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:38:27.214642Z","iopub.status.busy":"2023-10-30T05:38:27.214014Z","iopub.status.idle":"2023-10-30T05:38:28.031899Z","shell.execute_reply":"2023-10-30T05:38:28.030518Z","shell.execute_reply.started":"2023-10-30T05:38:27.214613Z"},"trusted":true},"outputs":[],"source":["from huggingface_hub import HfFolder\n","from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n","\n","# Hugging Face repository id \n","repository_id = \"results/\" + model_id.split(\"/\")[1]\n","\n","# Define training args\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=repository_id,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    predict_with_generate=True,\n","    fp16=False, # Overflows with fp16\n","    learning_rate=9e-5,\n","    num_train_epochs=150,\n","    # logging & evaluation strategies\n","    logging_dir=f\"{repository_id}/logs\",\n","    logging_strategy=\"steps\",\n","    logging_steps=1000,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    save_total_limit=2,\n","    load_best_model_at_end=True,\n","    # metric_for_best_model=\"overall_f1\",\n","    # push to hub parameters\n","    report_to=\"tensorboard\",\n","    push_to_hub=False,\n","    hub_strategy=\"every_save\",\n","    hub_model_id=repository_id,\n","    hub_token=HfFolder.get_token(),\n",")\n","\n","# Create Trainer instance\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=tokenized_dataset,\n","    eval_dataset=tokenized_dataset,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:38:28.034086Z","iopub.status.busy":"2023-10-30T05:38:28.033705Z","iopub.status.idle":"2023-10-30T05:38:28.058100Z","shell.execute_reply":"2023-10-30T05:38:28.057140Z","shell.execute_reply.started":"2023-10-30T05:38:28.034052Z"},"trusted":true},"outputs":[],"source":["# freeze the backbone and only finetune the decoder's second half.\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","# train end of encoder\n","for param in model.encoder.block[23:].parameters(): # total 11 block in base encoder, 23 in large\n","    param.requires_grad = True\n","for param in model.encoder.final_layer_norm.parameters():\n","    param.requires_grad = True\n","    \n","# train end of decoder\n","for param in model.decoder.block[23:].parameters(): #total 11 block in base decoder, 23 in large\n","    param.requires_grad = True\n","for param in model.decoder.final_layer_norm.parameters():\n","    param.requires_grad = True\n","    \n","for param in model.lm_head.parameters():\n","    param.requires_grad = True\n","\n","print(\"trainable_params / total_params\")\n","print(sum(p.numel() for p in model.parameters() if p.requires_grad), \"/\",sum(p.numel() for p in model.parameters()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:38:35.944391Z","iopub.status.busy":"2023-10-30T05:38:35.943760Z","iopub.status.idle":"2023-10-30T05:43:02.670853Z","shell.execute_reply":"2023-10-30T05:43:02.669277Z","shell.execute_reply.started":"2023-10-30T05:38:35.944360Z"},"trusted":true},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:43:06.674353Z","iopub.status.busy":"2023-10-30T05:43:06.673831Z","iopub.status.idle":"2023-10-30T05:43:15.099666Z","shell.execute_reply":"2023-10-30T05:43:15.098636Z","shell.execute_reply.started":"2023-10-30T05:43:06.674316Z"},"trusted":true},"outputs":[],"source":["trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:43:15.101599Z","iopub.status.busy":"2023-10-30T05:43:15.101306Z","iopub.status.idle":"2023-10-30T05:43:28.054702Z","shell.execute_reply":"2023-10-30T05:43:28.053568Z","shell.execute_reply.started":"2023-10-30T05:43:15.101574Z"},"trusted":true},"outputs":[],"source":["# Save our tokenizer and create model card\n","tokenizer.save_pretrained(\"salient-ai\")\n","model.save_pretrained(\"salient-ai\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:43:28.057204Z","iopub.status.busy":"2023-10-30T05:43:28.056582Z","iopub.status.idle":"2023-10-30T05:45:04.982983Z","shell.execute_reply":"2023-10-30T05:45:04.982002Z","shell.execute_reply.started":"2023-10-30T05:43:28.057164Z"},"trusted":true},"outputs":[],"source":["model_name_on_hub = \"Salient_ai\" + model_id.split(\"/\")[1]\n","tokenizer.push_to_hub(model_name_on_hub)\n","model.push_to_hub(model_name_on_hub)"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:45:04.985452Z","iopub.status.busy":"2023-10-30T05:45:04.985150Z","iopub.status.idle":"2023-10-30T05:45:57.377620Z","shell.execute_reply":"2023-10-30T05:45:57.376680Z","shell.execute_reply.started":"2023-10-30T05:45:04.985425Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModel\n","model_path = \"pratt3000/\" + model_name_on_hub\n","\n","model = AutoModel.from_pretrained(model_path)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:48:08.127312Z","iopub.status.busy":"2023-10-30T05:48:08.126892Z","iopub.status.idle":"2023-10-30T05:48:16.727394Z","shell.execute_reply":"2023-10-30T05:48:16.726582Z","shell.execute_reply.started":"2023-10-30T05:48:08.127278Z"},"trusted":true},"outputs":[],"source":["from transformers import pipeline\n","from random import randrange        \n","\n","# load model and tokenizer from huggingface hub with pipeline\n","model = pipeline(model=model_path)\n","    \n","# select a random test sample\n","sample = data_test_processed[randrange(len(data_test_processed))]\n","print(f\"dialogue: \\n{sample['prompt']}\\n---------------\")\n","\n","# summarize dialogue\n","res = model(sample[\"prompt\"])\n","\n","print(f\"RESULT: \\n{res}, ACTUAL: {sample['label']}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:48:16.729837Z","iopub.status.busy":"2023-10-30T05:48:16.729338Z","iopub.status.idle":"2023-10-30T05:48:37.823015Z","shell.execute_reply":"2023-10-30T05:48:37.822034Z","shell.execute_reply.started":"2023-10-30T05:48:16.729799Z"},"trusted":true},"outputs":[],"source":["model = pipeline(model = model_path)\n","\n","for id, d in enumerate(data_test_processed):\n","    if id<50:\n","        continue\n","    \n","    print(d['prompt'])\n","\n","    res = model(d[\"prompt\"])\n","\n","    print(f\"RESULT: \\n{res}, ACTUAL: {d['label']} \\n\\n\")\n","\n","    if id > 60:\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:48:37.824550Z","iopub.status.busy":"2023-10-30T05:48:37.824237Z","iopub.status.idle":"2023-10-30T05:48:37.832542Z","shell.execute_reply":"2023-10-30T05:48:37.831743Z","shell.execute_reply.started":"2023-10-30T05:48:37.824522Z"},"trusted":true},"outputs":[],"source":["from datetime import datetime\n","\n","def date_difference_in_days(date_str1, date_str2):\n","    \n","    if label_type == \"days_diff\":\n","        return abs(int(date_str1) - int(date_str2))\n","    \n","    # Define the format of the date string\n","    date_format = \"%Y-%m-%d\"\n","\n","    # Parse the date strings into datetime objects\n","    date1 = datetime.strptime(date_str1, date_format)\n","    date2 = datetime.strptime(date_str2, date_format)\n","\n","    # Calculate the difference in days\n","    delta = date2 - date1\n","    return abs(delta.days)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:48:37.834629Z","iopub.status.busy":"2023-10-30T05:48:37.834292Z","iopub.status.idle":"2023-10-30T05:51:33.269972Z","shell.execute_reply":"2023-10-30T05:51:33.269033Z","shell.execute_reply.started":"2023-10-30T05:48:37.834590Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","same = 0\n","cur_dist = 0\n","num_exceptions= 0\n","for id, d in tqdm(enumerate(data_train_processed)):\n","    res = model(d[\"prompt\"])\n","    \n","    if res[0]['generated_text'] == str(d['label']):\n","        same += 1\n","        cur_dist += 0\n","    elif res[0]['generated_text'] == 'NA' or str(d['label']) == 'NA':\n","        cur_dist += 10 # arbitrary 10 day error added\n","    else:\n","        try:\n","            cur_dist += date_difference_in_days(res[0]['generated_text'], str(d['label']))\n","        except Exception as e:\n","            print(e)\n","            print(res[0]['generated_text'], str(d['label']))\n","\n","print(\"ACCURACY (train) = \", same/(len(data_train_processed)-num_exceptions))\n","print(\"avg_deviation (train) = \", cur_dist/(len(data_train_processed)-num_exceptions))\n","\n","from tqdm import tqdm \n","same = 0\n","cur_dist = 0\n","num_exceptions= 0\n","for id, d in tqdm(enumerate(data_test_processed)):\n","    res = model(d[\"prompt\"])\n","    \n","    if res[0]['generated_text'] == str(d['label']):\n","        same += 1\n","        cur_dist += 0\n","    elif res[0]['generated_text'] == 'NA' or str(d['label']) == 'NA':\n","        print(\"Wrong prediction: \")\n","        print(\"Generated date\", res[0]['generated_text'],\":: ground truth date\", str(d['label']))\n","        cur_dist += 10 # arbitrary 10 day error added\n","    else:\n","        try:\n","            print(\"Wrong prediction: \")\n","            print(\"Generated date\", res[0]['generated_text'],\":: ground truth date\", str(d['label']))\n","            cur_dist += date_difference_in_days(res[0]['generated_text'], str(d['label']))\n","        except Exception as e:\n","            print(e)\n","            print(\"Generated date\", res[0]['generated_text'],\":: ground truth date\", str(d['label']))\n","\n","print(\"ACCURACY (test) = \", same/(len(data_test_processed) - num_exceptions))\n","print(\"avg_deviation (test) = \", cur_dist/(len(data_test_processed) - num_exceptions))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
