{"cells":[{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T04:05:04.755789Z","iopub.status.busy":"2023-10-30T04:05:04.755414Z","iopub.status.idle":"2023-10-30T04:05:04.760878Z","shell.execute_reply":"2023-10-30T04:05:04.759760Z","shell.execute_reply.started":"2023-10-30T04:05:04.755759Z"},"trusted":true},"outputs":[],"source":["dataset_id = \"dat_pred\"\n","from huggingface_hub import notebook_login\n","# notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install evaluate\n","!pip install rouge_score"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T04:05:04.765468Z","iopub.status.busy":"2023-10-30T04:05:04.765214Z","iopub.status.idle":"2023-10-30T04:05:06.185120Z","shell.execute_reply":"2023-10-30T04:05:06.184047Z","shell.execute_reply.started":"2023-10-30T04:05:04.765445Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\""]},{"cell_type":"code","execution_count":100,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T04:34:10.468704Z","iopub.status.busy":"2023-10-30T04:34:10.468308Z","iopub.status.idle":"2023-10-30T04:34:11.918931Z","shell.execute_reply":"2023-10-30T04:34:11.918014Z","shell.execute_reply.started":"2023-10-30T04:34:10.468671Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5ba4f94e979e40dfad9feba741cd9f54","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84a9307b588f4819b288cea2777a7778","version_major":2,"version_minor":0},"text/plain":["Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b78ccae62b549edad64cc79a5a19821","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f1df7a98a5b94083bcd641bd433235e8","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","model_id=\"google/flan-t5-large\"\n","\n","# Load tokenizer of FLAN-t5\n","tokenizer = AutoTokenizer.from_pretrained(model_id)"]},{"cell_type":"code","execution_count":138,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:37:48.444931Z","iopub.status.busy":"2023-10-30T05:37:48.444015Z","iopub.status.idle":"2023-10-30T05:37:48.465194Z","shell.execute_reply":"2023-10-30T05:37:48.464160Z","shell.execute_reply.started":"2023-10-30T05:37:48.444897Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["original train size =  109 :: original test size =  113\n","train size =  159 :: test size =  63\n"]}],"source":["#read json\n","import json\n","import pandas as pd\n","from datasets import Dataset\n","\n","with open('/kaggle/input/salient/test_data.json') as f:\n","    test = json.load(f)\n","with open('/kaggle/input/salient-train/chatgpt_gen_date.json') as f:\n","    train = json.load(f)\n","    \n","import random\n","random.shuffle(train)\n","random.shuffle(test)\n","\n","print(\"original train size = \", len(train), \":: original test size = \", len(test))\n","train.extend(test[:50])\n","data_train = train\n","data_test = test[50:]\n","print(\"train size = \", len(data_train), \":: test size = \", len(data_test))\n","\n","data_train_processed = []\n","data_test_processed = []\n","# base_prompt = \"\"\"Given the above transcript and today's day and date, tell me after how many days will customer be able to pay?\n","# Return 'NA' if its not possible to infer that. Just output the number of days or 'NA' in your response and nothing else.\"\"\"\n","base_prompts_list = {\n","    \"label\": \"Given the above transcript and today's day and date, give me the date when the customer is expected to make their payment in the format 'dd/mm/yyyy'. Return 'NA' if its not possible to infer this information from the conversation. just return the date or NA and nothing else.\", \n","    \"days_diff\": \"Given the above transcript and today's day and date, give me the number of days after which the customer will be able to pay. Return 0 if its not possible to infer this information from the conversation.Just return the number of days or 0 if not inferrable and nothing else.\"\n","}\n","label_type_list = ['label', 'days_diff']\n","\n","label_type = label_type_list[1] \n","base_prompt = base_prompts_list[label_type]\n","\n","for d in data_train:\n","    data_train_processed.append(\n","        {\n","            'prompt': d['conversation'] + \"\\n todays date (dd/mm/yyyy) = \" + d['conversation_date'] + \"\\n\" + base_prompt,\n","            'label': str(d[label_type])\n","        }\n","    )\n","for d in data_test:\n","    data_test_processed.append(\n","        {\n","            'prompt': d['conversation'] + \"\\n todays date (dd/mm/yyyy) = \" + d['conversation_date'] + \"\\n\" + base_prompt,\n","            'label': str(d[label_type])\n","        }\n","    )\n","\n","data_train_processed = Dataset.from_pandas(pd.DataFrame(data=data_train_processed))\n","data_test_processed = Dataset.from_pandas(pd.DataFrame(data=data_test_processed))\n","\n","# dataset = {\"train\": data_train_processed, \"test\":data_test_processed}"]},{"cell_type":"code","execution_count":139,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:37:52.111754Z","iopub.status.busy":"2023-10-30T05:37:52.110935Z","iopub.status.idle":"2023-10-30T05:37:52.239643Z","shell.execute_reply":"2023-10-30T05:37:52.238513Z","shell.execute_reply.started":"2023-10-30T05:37:52.111718Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"063b8353f5084ab583d4746d82712b8f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Max source length: 148\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"96a75cafec484617b2e5107472a15b62","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Max target length: 3\n"]}],"source":["from datasets import concatenate_datasets\n","\n","# The maximum total input sequence length after tokenization. \n","# Sequences longer than this will be truncated, sequences shorter will be padded.\n","tokenized_inputs = concatenate_datasets([data_train_processed, data_train_processed]).map(lambda x: tokenizer(x[\"prompt\"], truncation=True), batched=True, remove_columns=[\"prompt\", \"label\"])\n","max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n","print(f\"Max source length: {max_source_length}\")\n","\n","# The maximum total sequence length for target text after tokenization. \n","# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n","tokenized_targets = concatenate_datasets([data_train_processed, data_train_processed]).map(lambda x: tokenizer(x[\"label\"], truncation=True), batched=True, remove_columns=[\"prompt\", \"label\"])\n","max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n","print(f\"Max target length: {max_target_length}\")"]},{"cell_type":"code","execution_count":140,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:37:59.214069Z","iopub.status.busy":"2023-10-30T05:37:59.213647Z","iopub.status.idle":"2023-10-30T05:37:59.278155Z","shell.execute_reply":"2023-10-30T05:37:59.277256Z","shell.execute_reply.started":"2023-10-30T05:37:59.214036Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7177976cbd61427aa6907037148a374e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"]}],"source":["def preprocess_function(sample,padding=\"max_length\"):\n","    # add prefix to the input for t5\n","    inputs = [item for item in sample[\"prompt\"]]\n","\n","    # tokenize inputs\n","    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n","\n","    # Tokenize targets with the `text_target` keyword argument\n","    labels = tokenizer(text_target=sample[\"label\"], max_length=max_target_length, padding=padding, truncation=True)\n","\n","    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n","    # padding in the loss.\n","    if padding == \"max_length\":\n","        labels[\"input_ids\"] = [\n","            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n","        ]\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","tokenized_dataset = data_train_processed.map(preprocess_function, batched=True, remove_columns=[\"prompt\", \"label\"])\n","print(f\"Keys of tokenized dataset: {list(tokenized_dataset.features)}\")"]},{"cell_type":"code","execution_count":141,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:38:00.505069Z","iopub.status.busy":"2023-10-30T05:38:00.504225Z","iopub.status.idle":"2023-10-30T05:38:26.495469Z","shell.execute_reply":"2023-10-30T05:38:26.494549Z","shell.execute_reply.started":"2023-10-30T05:38:00.505031Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForSeq2SeqLM\n","\n","# load model from the hub\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"]},{"cell_type":"code","execution_count":142,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:38:26.498199Z","iopub.status.busy":"2023-10-30T05:38:26.497816Z","iopub.status.idle":"2023-10-30T05:38:27.205508Z","shell.execute_reply":"2023-10-30T05:38:27.204618Z","shell.execute_reply.started":"2023-10-30T05:38:26.498164Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import evaluate\n","import nltk\n","import numpy as np\n","from nltk.tokenize import sent_tokenize\n","nltk.download(\"punkt\")\n","\n","# Metric\n","metric = evaluate.load(\"rouge\")\n","\n","# helper function to postprocess text\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [label.strip() for label in labels]\n","\n","    # rougeLSum expects newline after each sentence\n","    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n","    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n","\n","    return preds, labels\n","\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Some simple post-processing\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    result = {k: round(v * 100, 4) for k, v in result.items()}\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    return result"]},{"cell_type":"code","execution_count":143,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:38:27.206879Z","iopub.status.busy":"2023-10-30T05:38:27.206609Z","iopub.status.idle":"2023-10-30T05:38:27.211746Z","shell.execute_reply":"2023-10-30T05:38:27.210832Z","shell.execute_reply.started":"2023-10-30T05:38:27.206855Z"},"trusted":true},"outputs":[],"source":["from transformers import DataCollatorForSeq2Seq\n","\n","# we want to ignore tokenizer pad token in the loss\n","label_pad_token_id = -100\n","# Data collator\n","data_collator = DataCollatorForSeq2Seq(\n","    tokenizer,\n","    model=model,\n","    label_pad_token_id=label_pad_token_id,\n","    pad_to_multiple_of=8\n",")"]},{"cell_type":"code","execution_count":144,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:38:27.214642Z","iopub.status.busy":"2023-10-30T05:38:27.214014Z","iopub.status.idle":"2023-10-30T05:38:28.031899Z","shell.execute_reply":"2023-10-30T05:38:28.030518Z","shell.execute_reply.started":"2023-10-30T05:38:27.214613Z"},"trusted":true},"outputs":[],"source":["from huggingface_hub import HfFolder\n","from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n","\n","# Hugging Face repository id \n","repository_id = \"results/\" + model_id.split(\"/\")[1]\n","\n","# Define training args\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=repository_id,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    predict_with_generate=True,\n","    fp16=False, # Overflows with fp16\n","    learning_rate=9e-5,\n","    num_train_epochs=150,\n","    # logging & evaluation strategies\n","    logging_dir=f\"{repository_id}/logs\",\n","    logging_strategy=\"steps\",\n","    logging_steps=1000,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    save_total_limit=2,\n","    load_best_model_at_end=True,\n","    # metric_for_best_model=\"overall_f1\",\n","    # push to hub parameters\n","    report_to=\"tensorboard\",\n","    push_to_hub=False,\n","    hub_strategy=\"every_save\",\n","    hub_model_id=repository_id,\n","    hub_token=HfFolder.get_token(),\n",")\n","\n","# Create Trainer instance\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=tokenized_dataset,\n","    eval_dataset=tokenized_dataset,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":145,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:38:28.034086Z","iopub.status.busy":"2023-10-30T05:38:28.033705Z","iopub.status.idle":"2023-10-30T05:38:28.058100Z","shell.execute_reply":"2023-10-30T05:38:28.057140Z","shell.execute_reply.started":"2023-10-30T05:38:28.034052Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable_params / total_params\n","62790656 / 783150080\n"]}],"source":["# freeze the backbone and only finetune the decoder's second half.\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","# train end of encoder\n","for param in model.encoder.block[23:].parameters(): # total 11 block in base encoder, 23 in large\n","    param.requires_grad = True\n","for param in model.encoder.final_layer_norm.parameters():\n","    param.requires_grad = True\n","    \n","# train end of decoder\n","for param in model.decoder.block[23:].parameters(): #total 11 block in base decoder, 23 in large\n","    param.requires_grad = True\n","for param in model.decoder.final_layer_norm.parameters():\n","    param.requires_grad = True\n","    \n","for param in model.lm_head.parameters():\n","    param.requires_grad = True\n","\n","print(\"trainable_params / total_params\")\n","print(sum(p.numel() for p in model.parameters() if p.requires_grad), \"/\",sum(p.numel() for p in model.parameters()))\n"]},{"cell_type":"code","execution_count":146,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:38:35.944391Z","iopub.status.busy":"2023-10-30T05:38:35.943760Z","iopub.status.idle":"2023-10-30T05:43:02.670853Z","shell.execute_reply":"2023-10-30T05:43:02.669277Z","shell.execute_reply.started":"2023-10-30T05:38:35.944360Z"},"trusted":true},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='261' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 261/1000 04:13 < 12:04, 1.02 it/s, Epoch 13/50]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge1</th>\n","      <th>Rouge2</th>\n","      <th>Rougel</th>\n","      <th>Rougelsum</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>1.737809</td>\n","      <td>10.691800</td>\n","      <td>0.000000</td>\n","      <td>10.691800</td>\n","      <td>10.691800</td>\n","      <td>2.081761</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>1.620922</td>\n","      <td>12.578600</td>\n","      <td>0.000000</td>\n","      <td>12.578600</td>\n","      <td>12.578600</td>\n","      <td>2.106918</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>1.565258</td>\n","      <td>14.465400</td>\n","      <td>0.000000</td>\n","      <td>14.465400</td>\n","      <td>14.465400</td>\n","      <td>2.138365</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>No log</td>\n","      <td>1.544819</td>\n","      <td>15.723300</td>\n","      <td>0.000000</td>\n","      <td>15.723300</td>\n","      <td>15.723300</td>\n","      <td>2.238994</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>No log</td>\n","      <td>1.513226</td>\n","      <td>16.352200</td>\n","      <td>0.000000</td>\n","      <td>16.352200</td>\n","      <td>16.352200</td>\n","      <td>2.427673</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>No log</td>\n","      <td>1.493467</td>\n","      <td>17.610100</td>\n","      <td>0.000000</td>\n","      <td>17.610100</td>\n","      <td>17.610100</td>\n","      <td>2.408805</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>No log</td>\n","      <td>1.480512</td>\n","      <td>17.610100</td>\n","      <td>0.000000</td>\n","      <td>17.610100</td>\n","      <td>17.610100</td>\n","      <td>2.421384</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>No log</td>\n","      <td>1.470374</td>\n","      <td>16.981100</td>\n","      <td>0.000000</td>\n","      <td>16.981100</td>\n","      <td>16.981100</td>\n","      <td>2.320755</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>No log</td>\n","      <td>1.453409</td>\n","      <td>16.981100</td>\n","      <td>0.000000</td>\n","      <td>16.981100</td>\n","      <td>16.981100</td>\n","      <td>2.308176</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>No log</td>\n","      <td>1.436772</td>\n","      <td>17.610100</td>\n","      <td>0.000000</td>\n","      <td>17.610100</td>\n","      <td>17.610100</td>\n","      <td>2.276730</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>No log</td>\n","      <td>1.426958</td>\n","      <td>18.239000</td>\n","      <td>0.000000</td>\n","      <td>18.239000</td>\n","      <td>18.239000</td>\n","      <td>2.163522</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>No log</td>\n","      <td>1.410433</td>\n","      <td>19.496900</td>\n","      <td>0.000000</td>\n","      <td>19.496900</td>\n","      <td>19.496900</td>\n","      <td>2.194969</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>No log</td>\n","      <td>1.401981</td>\n","      <td>18.867900</td>\n","      <td>0.000000</td>\n","      <td>18.867900</td>\n","      <td>18.867900</td>\n","      <td>2.150943</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>No log</td>\n","      <td>1.401981</td>\n","      <td>18.867900</td>\n","      <td>0.000000</td>\n","      <td>18.867900</td>\n","      <td>18.867900</td>\n","      <td>2.150943</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[146], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1942\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1939\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1941\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 1942\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   1945\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   1946\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2265\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2262\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[1;32m   2264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2265\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2322\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2320\u001b[0m run_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_dir(trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   2321\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, checkpoint_folder)\n\u001b[0;32m-> 2322\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled:\n\u001b[1;32m   2324\u001b[0m     \u001b[38;5;66;03m# under zero3 model file itself doesn't get saved since it's bogus! Unless deepspeed\u001b[39;00m\n\u001b[1;32m   2325\u001b[0m     \u001b[38;5;66;03m# config `stage3_gather_16bit_weights_on_model_save` is True\u001b[39;00m\n\u001b[1;32m   2326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2803\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   2800\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   2802\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2803\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2805\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   2806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2861\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   2859\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[1;32m   2860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2862\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_safetensors\u001b[49m\n\u001b[1;32m   2863\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2866\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1988\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   1986\u001b[0m         safe_save_file(shard, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, shard_file), metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m   1987\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1988\u001b[0m         \u001b[43msave_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1991\u001b[0m     path_to_weights \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, _add_variant(WEIGHTS_NAME, variant))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:441\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 441\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:668\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m    667\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 668\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":147,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:43:06.674353Z","iopub.status.busy":"2023-10-30T05:43:06.673831Z","iopub.status.idle":"2023-10-30T05:43:15.099666Z","shell.execute_reply":"2023-10-30T05:43:15.098636Z","shell.execute_reply.started":"2023-10-30T05:43:06.674316Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'eval_loss': 1.401981234550476,\n"," 'eval_rouge1': 18.8679,\n"," 'eval_rouge2': 0.0,\n"," 'eval_rougeL': 18.8679,\n"," 'eval_rougeLsum': 18.8679,\n"," 'eval_gen_len': 2.150943396226415}"]},"execution_count":147,"metadata":{},"output_type":"execute_result"}],"source":["trainer.evaluate()"]},{"cell_type":"code","execution_count":148,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:43:15.101599Z","iopub.status.busy":"2023-10-30T05:43:15.101306Z","iopub.status.idle":"2023-10-30T05:43:28.054702Z","shell.execute_reply":"2023-10-30T05:43:28.053568Z","shell.execute_reply.started":"2023-10-30T05:43:15.101574Z"},"trusted":true},"outputs":[],"source":["# Save our tokenizer and create model card\n","tokenizer.save_pretrained(\"salient-ai\")\n","model.save_pretrained(\"salient-ai\")"]},{"cell_type":"code","execution_count":149,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:43:28.057204Z","iopub.status.busy":"2023-10-30T05:43:28.056582Z","iopub.status.idle":"2023-10-30T05:45:04.982983Z","shell.execute_reply":"2023-10-30T05:45:04.982002Z","shell.execute_reply.started":"2023-10-30T05:43:28.057164Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0aed5ba336304876b3b018fa63d6da88","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/pratt3000/Salient_aiflan-t5-large/commit/6439afc85f39cf94b98d722e3a3604ab862ff850', commit_message='Upload T5ForConditionalGeneration', commit_description='', oid='6439afc85f39cf94b98d722e3a3604ab862ff850', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":149,"metadata":{},"output_type":"execute_result"}],"source":["model_name_on_hub = \"Salient_ai\" + model_id.split(\"/\")[1]\n","tokenizer.push_to_hub(model_name_on_hub)\n","model.push_to_hub(model_name_on_hub)"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":150,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:45:04.985452Z","iopub.status.busy":"2023-10-30T05:45:04.985150Z","iopub.status.idle":"2023-10-30T05:45:57.377620Z","shell.execute_reply":"2023-10-30T05:45:57.376680Z","shell.execute_reply.started":"2023-10-30T05:45:04.985425Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a51e3bd612c47d99b208ac0a6ba5c94","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoModel\n","model_path = \"pratt3000/\" + model_name_on_hub\n","\n","model = AutoModel.from_pretrained(model_path)\n","\n"]},{"cell_type":"code","execution_count":152,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:48:08.127312Z","iopub.status.busy":"2023-10-30T05:48:08.126892Z","iopub.status.idle":"2023-10-30T05:48:16.727394Z","shell.execute_reply":"2023-10-30T05:48:16.726582Z","shell.execute_reply.started":"2023-10-30T05:48:08.127278Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["dialogue: \n","Agent: Hello, this is Westlake Financial. Could you please verify your date of birth for security purposes?\n","Customer: I believe you have the wrong person, I don't have any financial dealings with you.\n"," todays date (dd/mm/yyyy) = 2022-03-14, Monday\n","Given the above transcript and today's day and date, give me the number of days after which the customer will be able to pay. Return 0 if its not possible to infer this information from the conversation.\n","---------------\n","RESULT: \n","[{'generated_text': '0'}], ACTUAL: 0\n"]}],"source":["from transformers import pipeline\n","from random import randrange        \n","\n","# load model and tokenizer from huggingface hub with pipeline\n","model = pipeline(model=model_path)\n","    \n","# select a random test sample\n","sample = data_test_processed[randrange(len(data_test_processed))]\n","print(f\"dialogue: \\n{sample['prompt']}\\n---------------\")\n","\n","# summarize dialogue\n","res = model(sample[\"prompt\"])\n","\n","print(f\"RESULT: \\n{res}, ACTUAL: {sample['label']}\")"]},{"cell_type":"code","execution_count":153,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:48:16.729837Z","iopub.status.busy":"2023-10-30T05:48:16.729338Z","iopub.status.idle":"2023-10-30T05:48:37.823015Z","shell.execute_reply":"2023-10-30T05:48:37.822034Z","shell.execute_reply.started":"2023-10-30T05:48:16.729799Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Customer: not exactly\n","Agent: That's okay, Raymond. Could you give me a rough estimate? For example, would you be able to make the payment by the end of this week?\n","Customer: probably by the end of next week\n"," todays date (dd/mm/yyyy) = 2022-05-19, Thursday\n","Given the above transcript and today's day and date, give me the number of days after which the customer will be able to pay. Return 0 if its not possible to infer this information from the conversation.\n","RESULT: \n","[{'generated_text': '6'}], ACTUAL: 8 \n","\n","\n","Agent: Can you inform us about your preferred date of payment?\n","Customer: I can send the funds by next Tuesday. \n"," todays date (dd/mm/yyyy) = 2021-03-24, Wednesday\n","Given the above transcript and today's day and date, give me the number of days after which the customer will be able to pay. Return 0 if its not possible to infer this information from the conversation.\n","RESULT: \n","[{'generated_text': '6'}], ACTUAL: 6 \n","\n","\n","Agent: Are you able to make a payment today?\n","Customer: No, but I can definitely make the payment tomorrow.\n"," todays date (dd/mm/yyyy) = 2020-09-30, Wednesday\n","Given the above transcript and today's day and date, give me the number of days after which the customer will be able to pay. Return 0 if its not possible to infer this information from the conversation.\n","RESULT: \n","[{'generated_text': '2'}], ACTUAL: 1 \n","\n","\n","Agent: Hi, I'm Taylor, calling from Westlake Financial on a recorded line. Unfortunately, we did not receive your monthly payment! Would you be able to make a payment today?\n","Customer: yeah how much is the payment\n","Agent: Your monthly payment is $322.55. Are you able to make this payment today?\n","Customer: yeah i can make a payment sorry my wife normally does this but i yeah i can do a payment right now right now\n"," todays date (dd/mm/yyyy) = 2023-03-23, Thursday\n","Given the above transcript and today's day and date, give me the number of days after which the customer will be able to pay. Return 0 if its not possible to infer this information from the conversation.\n","RESULT: \n","[{'generated_text': '0'}], ACTUAL: 0 \n","\n","\n","Agent: ...and aside from that, do you have any other concerns or inquiries?\n","Customer: Yes, I do. Would it be possible to change my mailing address? I just moved.\n"," todays date (dd/mm/yyyy) = 2023-06-09, Friday\n","Given the above transcript and today's day and date, give me the number of days after which the customer will be able to pay. Return 0 if its not possible to infer this information from the conversation.\n","RESULT: \n","[{'generated_text': '0'}], ACTUAL: 0 \n","\n","\n","Agent: Can you make a payment by next Thursday?\n","Customer: Yes, I have marked it in my calendar.\n"," todays date (dd/mm/yyyy) = 2023-02-20, Monday\n","Given the above transcript and today's day and date, give me the number of days after which the customer will be able to pay. Return 0 if its not possible to infer this information from the conversation.\n","RESULT: \n","[{'generated_text': '6'}], ACTUAL: 10 \n","\n","\n","Agent: Can you pay us by the 12th of this month?\n","Customer: Yes, I can do that.\n"," todays date (dd/mm/yyyy) = 2022-10-10, Monday\n","Given the above transcript and today's day and date, give me the number of days after which the customer will be able to pay. Return 0 if its not possible to infer this information from the conversation.\n","RESULT: \n","[{'generated_text': '6'}], ACTUAL: 2 \n","\n","\n","Agent: I'm - That's great, Travis. Could you please specify the date when you can make the payment?\n","Customer: october the twenty sixth\n"," todays date (dd/mm/yyyy) = 2023-06-21, Wednesday\n","Given the above transcript and today's day and date, give me the number of days after which the customer will be able to pay. Return 0 if its not possible to infer this information from the conversation.\n","RESULT: \n","[{'generated_text': '6'}], ACTUAL: 127 \n","\n","\n","Agent: Are you able to make a payment at the end of the month?\n","Customer: I could make a payment at the beginning of next month.\n"," todays date (dd/mm/yyyy) = 2020-09-09, Wednesday\n","Given the above transcript and today's day and date, give me the number of days after which the customer will be able to pay. Return 0 if its not possible to infer this information from the conversation.\n","RESULT: \n","[{'generated_text': '6'}], ACTUAL: 22 \n","\n","\n","Agent: May I know when you will be able to make a payment?\n","Customer: I plan on paying on the 17th.\n"," todays date (dd/mm/yyyy) = 2022-04-10, Sunday\n","Given the above transcript and today's day and date, give me the number of days after which the customer will be able to pay. Return 0 if its not possible to infer this information from the conversation.\n","RESULT: \n","[{'generated_text': '6'}], ACTUAL: 7 \n","\n","\n","Agent: Do you think you could make a payment this week?\n","Customer: Easily within today or tomorrow. \n"," todays date (dd/mm/yyyy) = 2020-10-07, Wednesday\n","Given the above transcript and today's day and date, give me the number of days after which the customer will be able to pay. Return 0 if its not possible to infer this information from the conversation.\n","RESULT: \n","[{'generated_text': '2'}], ACTUAL: 1 \n","\n","\n","Agent: Could you tell me when will you be able to make a payment?\n","Customer: I can do it on the first of next month.\n"," todays date (dd/mm/yyyy) = 2023-06-07, Wednesday\n","Given the above transcript and today's day and date, give me the number of days after which the customer will be able to pay. Return 0 if its not possible to infer this information from the conversation.\n","RESULT: \n","[{'generated_text': '6'}], ACTUAL: 24 \n","\n","\n"]}],"source":["model = pipeline(model = model_path)\n","\n","for id, d in enumerate(data_test_processed):\n","    if id<50:\n","        continue\n","    \n","    print(d['prompt'])\n","\n","    res = model(d[\"prompt\"])\n","\n","    print(f\"RESULT: \\n{res}, ACTUAL: {d['label']} \\n\\n\")\n","\n","    if id > 60:\n","        break"]},{"cell_type":"code","execution_count":154,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:48:37.824550Z","iopub.status.busy":"2023-10-30T05:48:37.824237Z","iopub.status.idle":"2023-10-30T05:48:37.832542Z","shell.execute_reply":"2023-10-30T05:48:37.831743Z","shell.execute_reply.started":"2023-10-30T05:48:37.824522Z"},"trusted":true},"outputs":[],"source":["from datetime import datetime\n","\n","def date_difference_in_days(date_str1, date_str2):\n","    \n","    if label_type == \"days_diff\":\n","        return abs(int(date_str1) - int(date_str2))\n","    \n","    # Define the format of the date string\n","    date_format = \"%Y-%m-%d\"\n","\n","    # Parse the date strings into datetime objects\n","    date1 = datetime.strptime(date_str1, date_format)\n","    date2 = datetime.strptime(date_str2, date_format)\n","\n","    # Calculate the difference in days\n","    delta = date2 - date1\n","    return abs(delta.days)"]},{"cell_type":"code","execution_count":155,"metadata":{"execution":{"iopub.execute_input":"2023-10-30T05:48:37.834629Z","iopub.status.busy":"2023-10-30T05:48:37.834292Z","iopub.status.idle":"2023-10-30T05:51:33.269972Z","shell.execute_reply":"2023-10-30T05:51:33.269033Z","shell.execute_reply.started":"2023-10-30T05:48:37.834590Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["159it [02:04,  1.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["ACCURACY (train) =  0.18867924528301888\n","avg_deviation (train) =  7.9245283018867925\n"]},{"name":"stderr","output_type":"stream","text":["1it [00:00,  1.38it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 2 :: ground truth date 1\n"]},{"name":"stderr","output_type":"stream","text":["2it [00:01,  1.33it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 29\n"]},{"name":"stderr","output_type":"stream","text":["3it [00:02,  1.33it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 9\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:02,  1.34it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 8\n"]},{"name":"stderr","output_type":"stream","text":["5it [00:03,  1.37it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 2 :: ground truth date 1\n"]},{"name":"stderr","output_type":"stream","text":["9it [00:07,  1.25it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 17\n"]},{"name":"stderr","output_type":"stream","text":["12it [00:09,  1.08it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 0 :: ground truth date 1\n"]},{"name":"stderr","output_type":"stream","text":["13it [00:10,  1.15it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 16\n"]},{"name":"stderr","output_type":"stream","text":["14it [00:11,  1.18it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 31\n"]},{"name":"stderr","output_type":"stream","text":["15it [00:12,  1.21it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 4 :: ground truth date 0\n"]},{"name":"stderr","output_type":"stream","text":["16it [00:13,  1.25it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 2 :: ground truth date 3\n"]},{"name":"stderr","output_type":"stream","text":["18it [00:14,  1.25it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 15 :: ground truth date 38\n"]},{"name":"stderr","output_type":"stream","text":["19it [00:15,  1.28it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 2\n"]},{"name":"stderr","output_type":"stream","text":["20it [00:16,  1.28it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 29\n"]},{"name":"stderr","output_type":"stream","text":["21it [00:17,  1.15it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 0 :: ground truth date 3\n"]},{"name":"stderr","output_type":"stream","text":["23it [00:18,  1.23it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 3\n"]},{"name":"stderr","output_type":"stream","text":["24it [00:19,  1.20it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 0 :: ground truth date 1\n"]},{"name":"stderr","output_type":"stream","text":["25it [00:20,  1.23it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 1\n"]},{"name":"stderr","output_type":"stream","text":["26it [00:21,  1.27it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 37\n"]},{"name":"stderr","output_type":"stream","text":["27it [00:21,  1.28it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 4\n"]},{"name":"stderr","output_type":"stream","text":["30it [00:24,  1.28it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 4\n"]},{"name":"stderr","output_type":"stream","text":["32it [00:25,  1.30it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 15\n"]},{"name":"stderr","output_type":"stream","text":["33it [00:26,  1.31it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 5\n"]},{"name":"stderr","output_type":"stream","text":["34it [00:27,  1.30it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 3\n"]},{"name":"stderr","output_type":"stream","text":["35it [00:28,  1.31it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 4\n"]},{"name":"stderr","output_type":"stream","text":["36it [00:28,  1.33it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 11\n"]},{"name":"stderr","output_type":"stream","text":["38it [00:30,  1.31it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 29\n"]},{"name":"stderr","output_type":"stream","text":["39it [00:31,  1.31it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 9\n"]},{"name":"stderr","output_type":"stream","text":["40it [00:31,  1.29it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date -24\n"]},{"name":"stderr","output_type":"stream","text":["43it [00:34,  1.21it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 7\n"]},{"name":"stderr","output_type":"stream","text":["44it [00:35,  1.25it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 7\n"]},{"name":"stderr","output_type":"stream","text":["45it [00:36,  1.29it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 27\n"]},{"name":"stderr","output_type":"stream","text":["46it [00:36,  1.32it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 4 :: ground truth date 6\n"]},{"name":"stderr","output_type":"stream","text":["48it [00:38,  1.24it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 4 :: ground truth date 6\n"]},{"name":"stderr","output_type":"stream","text":["49it [00:39,  1.28it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 5\n"]},{"name":"stderr","output_type":"stream","text":["51it [00:41,  1.09it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 8\n"]},{"name":"stderr","output_type":"stream","text":["53it [00:42,  1.22it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 2 :: ground truth date 1\n"]},{"name":"stderr","output_type":"stream","text":["56it [00:45,  1.19it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 10\n"]},{"name":"stderr","output_type":"stream","text":["57it [00:46,  1.23it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 2\n"]},{"name":"stderr","output_type":"stream","text":["58it [00:47,  1.25it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 127\n"]},{"name":"stderr","output_type":"stream","text":["59it [00:47,  1.27it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 22\n"]},{"name":"stderr","output_type":"stream","text":["60it [00:48,  1.30it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 7\n"]},{"name":"stderr","output_type":"stream","text":["61it [00:49,  1.33it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 2 :: ground truth date 1\n"]},{"name":"stderr","output_type":"stream","text":["62it [00:49,  1.33it/s]"]},{"name":"stdout","output_type":"stream","text":["Wrong prediction: \n","Generated date 6 :: ground truth date 24\n"]},{"name":"stderr","output_type":"stream","text":["63it [00:50,  1.24it/s]"]},{"name":"stdout","output_type":"stream","text":["ACCURACY (test) =  0.30158730158730157\n","avg_deviation (test) =  7.158730158730159\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from tqdm import tqdm\n","same = 0\n","cur_dist = 0\n","num_exceptions= 0\n","for id, d in tqdm(enumerate(data_train_processed)):\n","    res = model(d[\"prompt\"])\n","    \n","    if res[0]['generated_text'] == str(d['label']):\n","        same += 1\n","        cur_dist += 0\n","    elif res[0]['generated_text'] == 'NA' or str(d['label']) == 'NA':\n","        cur_dist += 10 # arbitrary 10 day error added\n","    else:\n","        try:\n","            cur_dist += date_difference_in_days(res[0]['generated_text'], str(d['label']))\n","        except Exception as e:\n","            print(e)\n","            print(res[0]['generated_text'], str(d['label']))\n","\n","print(\"ACCURACY (train) = \", same/(len(data_train_processed)-num_exceptions))\n","print(\"avg_deviation (train) = \", cur_dist/(len(data_train_processed)-num_exceptions))\n","\n","from tqdm import tqdm \n","same = 0\n","cur_dist = 0\n","num_exceptions= 0\n","for id, d in tqdm(enumerate(data_test_processed)):\n","    res = model(d[\"prompt\"])\n","    \n","    if res[0]['generated_text'] == str(d['label']):\n","        same += 1\n","        cur_dist += 0\n","    elif res[0]['generated_text'] == 'NA' or str(d['label']) == 'NA':\n","        print(\"Wrong prediction: \")\n","        print(\"Generated date\", res[0]['generated_text'],\":: ground truth date\", str(d['label']))\n","        cur_dist += 10 # arbitrary 10 day error added\n","    else:\n","        try:\n","            print(\"Wrong prediction: \")\n","            print(\"Generated date\", res[0]['generated_text'],\":: ground truth date\", str(d['label']))\n","            cur_dist += date_difference_in_days(res[0]['generated_text'], str(d['label']))\n","        except Exception as e:\n","            print(e)\n","            print(\"Generated date\", res[0]['generated_text'],\":: ground truth date\", str(d['label']))\n","\n","print(\"ACCURACY (test) = \", same/(len(data_test_processed) - num_exceptions))\n","print(\"avg_deviation (test) = \", cur_dist/(len(data_test_processed) - num_exceptions))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
